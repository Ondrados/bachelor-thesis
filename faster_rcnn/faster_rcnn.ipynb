{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "BASE_DIR = config('BASE_DIR', default='', cast=str)\n",
    "dataset_path = os.path.join(BASE_DIR, 'data-science-bowl-2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        #image = np.resize(image, (new_h, new_w, 3))\n",
    "        image = cv2.resize(image, dsize=(new_h, new_w), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        ratio_height = new_h / h\n",
    "        ratio_width = new_w / w\n",
    "\n",
    "        xmin, ymin, xmax, ymax = targets[0][\"boxes\"].unbind(1)\n",
    "\n",
    "        xmin = xmin * ratio_width\n",
    "        xmax = xmax * ratio_width\n",
    "        ymin = ymin * ratio_height\n",
    "        ymax = ymax * ratio_height\n",
    "        \n",
    "        targets[0][\"boxes\"] = torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image).float()\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "        for t in self.transforms:\n",
    "            image, targets = t(image, targets)\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transforms=None, split=\"stage1_train\", path=dataset_path):\n",
    "        self.split = split\n",
    "        self.path = path + '/' + split\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.path_id_list = glob.glob(os.path.join(self.path, '*'))\n",
    "        self.id_list = []\n",
    "        self.image_list = []\n",
    "        self.mask_list = []\n",
    "\n",
    "        for path_id in self.path_id_list:\n",
    "            images = glob.glob(path_id + '/images/*png')\n",
    "            masks = glob.glob(path_id + '/masks/*png')\n",
    "            self.id_list.append(os.path.basename(path_id))\n",
    "            self.image_list.extend(images)\n",
    "            self.mask_list.append(masks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_id_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.array(Image.open(self.image_list[index]), dtype=np.uint8)\n",
    "        image = image[:, :, :3]  # remove alpha channel\n",
    "        boxes, labels = self.mask_to_bbox(self.mask_list[index])\n",
    "        targets = [\n",
    "            {\n",
    "                'boxes':torch.FloatTensor(boxes),\n",
    "                'labels':torch.LongTensor(labels),\n",
    "                'name': self.id_list[index]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        #sample = {'image': image, 'boxes': boxes, 'labels': labels, 'name': self.id_list[index]}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, targets = self.transforms(image, targets)\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    def mask_to_bbox(self, mask_paths):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for path in mask_paths:\n",
    "            mask = Image.open(path)\n",
    "            mask = np.array(mask)\n",
    "            pos = np.where(mask)\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(1)\n",
    "        #boxes = np.asarray(boxes,dtype=np.float32)\n",
    "        #labels = np.asarray(labels, dtype=np.int8)\n",
    "        return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train=False):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(Rescale((256,256)))\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(split='stage1_train', transforms=get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    image = batch[0]\n",
    "    target = [item[1] for item in batch]\n",
    "    return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from dataloader\n",
    "trainloader = DataLoader(dataset, num_workers=0, shuffle=True, drop_last=True, collate_fn=my_collate)\n",
    "it = iter(trainloader)\n",
    "image, targets = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 26.3111,  87.4667,  74.6667, 118.7556],\n",
      "        [  0.0000, 186.3111,  14.9333, 211.9111],\n",
      "        [ 22.0444, 123.7333,  68.2667, 160.7111],\n",
      "        [199.8222, 177.0667, 242.4889, 221.8667],\n",
      "        [ 82.4889, 205.5111,  86.7556, 208.3556],\n",
      "        [238.2222, 172.0889, 255.2889, 201.2444],\n",
      "        [145.7778,  17.7778, 185.6000,  53.3333],\n",
      "        [ 92.4445,  12.8000, 145.7778,  51.9111],\n",
      "        [132.9778,   0.0000, 159.2889,  15.6444],\n",
      "        [  0.0000, 107.3778,  22.7556, 138.6667],\n",
      "        [ 73.9556, 199.1111, 115.9111, 241.0667],\n",
      "        [238.9333, 119.4667, 255.2889, 149.3333],\n",
      "        [133.6889, 144.3556, 181.3333, 179.2000],\n",
      "        [ 86.0444, 246.7556, 109.5111, 255.2889],\n",
      "        [  0.0000,  71.1111,  11.3778,  93.8667],\n",
      "        [206.9333,   0.0000, 230.4000,  31.2889],\n",
      "        [184.8889,  20.6222, 233.9556,  56.1778],\n",
      "        [230.4000,   0.0000, 246.7556,  22.0444],\n",
      "        [121.6000,  98.8444, 167.1111, 142.9333]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'name': '89be66f88612aae541f5843abcd9c015832b5d6c54a28103b3019f7f38df8a6d'}]\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])\n",
    "print(image[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from dataset\n",
    "image, targets = dataset[0]\n",
    "image = image[None, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128),),\n",
    "                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be ['0']. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                             output_size=7,\n",
    "                                             sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(\n",
    "                backbone,\n",
    "                num_classes=2,\n",
    "                rpn_anchor_generator=anchor_generator,\n",
    "                box_roi_pool=roi_pooler,\n",
    "                min_size=256,\n",
    "                max_size=256,\n",
    "                  )\n",
    "\n",
    "#model.train()\n",
    "#loss = model(image, targets)\n",
    "\n",
    "#model.eval()\n",
    "#prediction = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01,\n",
    "                            momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1.5226142406463623\n",
      "1, 1.0527878999710083\n",
      "2, 0.8521511554718018\n",
      "3, 0.7292611598968506\n",
      "4, 0.6765128970146179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-b273515c0783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/Personal/cnn-cells/env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/Personal/cnn-cells/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):    \n",
    "    model.train()\n",
    "    loss = model(image, targets)\n",
    "    loss_sum = sum(lss for lss in loss.values())\n",
    "    optimizer.zero_grad()\n",
    "    loss_sum.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    prediction = model(image)\n",
    "    image2 = Image.fromarray(image.numpy()[0, 0, :, :])\n",
    "    if image2.mode != \"RGB\":\n",
    "        image2 = image2.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image2)\n",
    "    for box, score in zip(prediction[0][\"boxes\"], prediction[0][\"scores\"]):\n",
    "        x0, y0, x1, y1  = box\n",
    "        draw.rectangle([(x0, y0), (x1, y1)], outline=(255, 0, 255))\n",
    "\n",
    "    image2.show()\n",
    "\n",
    "    print(f\"{epoch}, {loss_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = FasterRCNN(\n",
    "                backbone,\n",
    "                num_classes=2,\n",
    "                rpn_anchor_generator=anchor_generator,\n",
    "                box_roi_pool=roi_pooler,\n",
    "                min_size=256,\n",
    "                max_size=256,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load('/Users/ondra/Desktop/faster_rcnn1.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load('/Users/ondra/Desktop/faster_rcnn1.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(Image.open(\"/Users/ondra/Dev/Personal/cnn-cells/data-science-bowl-2018/stage1_test/0a849e0eb15faa8a6d7329c3dd66aabe9a294cccb52ed30a90c8ca99092ae732/images/0a849e0eb15faa8a6d7329c3dd66aabe9a294cccb52ed30a90c8ca99092ae732.png\"),dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.transpose((2, 0, 1))\n",
    "image = torch.from_numpy(image).float()\n",
    "image = image[None, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss = model(image, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(nan, grad_fn=<NllLossBackward>),\n",
       " 'loss_box_reg': tensor(nan, grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(nan, grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(nan, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = Image.fromarray(image[0].numpy()[0, :, :])\n",
    "if image2.mode != \"RGB\":\n",
    "    image2 = image2.convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(image2)\n",
    "for box, score in zip(prediction[0][\"boxes\"], prediction[0][\"scores\"]):\n",
    "    x0, y0, x1, y1  = box\n",
    "    draw.rectangle([(x0, y0), (x1, y1)], outline=(255, 0, 255))\n",
    "\n",
    "image2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = Image.fromarray(image.numpy()[0, 0, :, :])\n",
    "if image3.mode != \"RGB\":\n",
    "    image3 = image3.convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(image3)\n",
    "for box in targets[0][\"boxes\"]:\n",
    "    x0, y0, x1, y1  = box\n",
    "    draw.rectangle([(x0, y0), (x1, y1)], outline=(255, 0, 255))\n",
    "\n",
    "image3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
