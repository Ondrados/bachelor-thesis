{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "BASE_DIR = config('BASE_DIR', default='', cast=str)\n",
    "dataset_path = os.path.join(BASE_DIR, 'data-science-bowl-2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        #image = np.resize(image, (new_h, new_w, 3))\n",
    "        image = cv2.resize(image, dsize=(new_h, new_w), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        ratio_height = new_h / h\n",
    "        ratio_width = new_w / w\n",
    "\n",
    "        xmin, ymin, xmax, ymax = targets[0][\"boxes\"].unbind(1)\n",
    "\n",
    "        xmin = xmin * ratio_width\n",
    "        xmax = xmax * ratio_width\n",
    "        ymin = ymin * ratio_height\n",
    "        ymax = ymax * ratio_height\n",
    "        \n",
    "        targets[0][\"boxes\"] = torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image).float()\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "        for t in self.transforms:\n",
    "            image, targets = t(image, targets)\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transforms=None, split=\"stage1_train\", path=dataset_path):\n",
    "        self.split = split\n",
    "        self.path = path + '/' + split\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.path_id_list = glob.glob(os.path.join(self.path, '*'))\n",
    "        self.id_list = []\n",
    "        self.image_list = []\n",
    "        self.mask_list = []\n",
    "\n",
    "        for path_id in self.path_id_list:\n",
    "            images = glob.glob(path_id + '/images/*png')\n",
    "            masks = glob.glob(path_id + '/masks/*png')\n",
    "            self.id_list.append(os.path.basename(path_id))\n",
    "            self.image_list.extend(images)\n",
    "            self.mask_list.append(masks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_id_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.array(Image.open(self.image_list[index]), dtype=np.uint8)\n",
    "        image = image[:, :, :3]  # remove alpha channel\n",
    "        boxes, labels = self.mask_to_bbox(self.mask_list[index])\n",
    "        targets = [\n",
    "            {\n",
    "                'boxes':torch.FloatTensor(boxes),\n",
    "                'labels':torch.LongTensor(labels),\n",
    "                'name': self.id_list[index]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        #sample = {'image': image, 'boxes': boxes, 'labels': labels, 'name': self.id_list[index]}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, targets = self.transforms(image, targets)\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    def mask_to_bbox(self, mask_paths):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for path in mask_paths:\n",
    "            mask = Image.open(path)\n",
    "            mask = np.array(mask)\n",
    "            pos = np.where(mask)\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(1)\n",
    "        #boxes = np.asarray(boxes,dtype=np.float32)\n",
    "        #labels = np.asarray(labels, dtype=np.int8)\n",
    "        return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train=False):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(Rescale((256,256)))\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(split='stage1_train', transforms=get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    image = batch[0]\n",
    "    target = [item[1] for item in batch]\n",
    "    return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from dataloader\n",
    "trainloader = DataLoader(dataset, num_workers=0, shuffle=True, drop_last=True, collate_fn=my_collate)\n",
    "it = iter(trainloader)\n",
    "image, targets = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 84.5283, 137.5522,  86.9434, 141.3731],\n",
      "        [184.9560, 180.4312, 187.9748, 191.4693],\n",
      "        [ 72.4528, 135.0050,  74.2641, 138.8259],\n",
      "        ...,\n",
      "        [125.3836, 181.7048, 127.3962, 187.6484],\n",
      "        [ 83.9245, 106.5605,  85.5346, 110.3814],\n",
      "        [105.8616, 169.3930, 107.4717, 172.3648]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'name': 'a102535b0e88374bea4a1cfd9ee7cb3822ff54f4ab2a9845d428ec22f9ee2288'}]\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])\n",
    "print(image[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from dataset\n",
    "image, targets = dataset[0]\n",
    "image = image[None, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-384041aed51e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtargets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "for image, targets in (dataset):\n",
    "    print(targets[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128),),\n",
    "                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be ['0']. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                             output_size=7,\n",
    "                                             sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(\n",
    "                backbone,\n",
    "                num_classes=2,\n",
    "                rpn_anchor_generator=anchor_generator,\n",
    "                box_roi_pool=roi_pooler,\n",
    "                min_size=256,\n",
    "                max_size=256,\n",
    "                  )\n",
    "\n",
    "#model.train()\n",
    "#loss = model(image, targets)\n",
    "\n",
    "#model.eval()\n",
    "#prediction = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01,\n",
    "                            momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1.5226142406463623\n",
      "1, 1.0527878999710083\n",
      "2, 0.8521511554718018\n",
      "3, 0.7292611598968506\n",
      "4, 0.6765128970146179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-171-b273515c0783>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mloss_sum\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlss\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlss\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mloss_sum\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dev/Personal/cnn-cells/env/lib/python3.7/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    193\u001B[0m                 \u001B[0mproducts\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    194\u001B[0m         \"\"\"\n\u001B[0;32m--> 195\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dev/Personal/cnn-cells/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m     97\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m     98\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):    \n",
    "    model.train()\n",
    "    loss = model(image, targets)\n",
    "    loss_sum = sum(lss for lss in loss.values())\n",
    "    optimizer.zero_grad()\n",
    "    loss_sum.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    prediction = model(image)\n",
    "    image2 = Image.fromarray(image.numpy()[0, 0, :, :])\n",
    "    if image2.mode != \"RGB\":\n",
    "        image2 = image2.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image2)\n",
    "    for box, score in zip(prediction[0][\"boxes\"], prediction[0][\"scores\"]):\n",
    "        x0, y0, x1, y1  = box\n",
    "        draw.rectangle([(x0, y0), (x1, y1)], outline=(255, 0, 255))\n",
    "\n",
    "    image2.show()\n",
    "\n",
    "    print(f\"{epoch}, {loss_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prediction = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[1.4711e+02, 2.1045e+02, 1.9949e+02, 2.4327e+02],\n",
       "          [1.3220e+02, 2.1952e+02, 2.1291e+02, 2.5600e+02],\n",
       "          [7.1682e+01, 1.3358e+00, 1.9804e+02, 8.5147e+01],\n",
       "          [1.5647e+02, 2.1592e+02, 1.6529e+02, 2.3014e+02],\n",
       "          [1.5531e+02, 2.1968e+02, 1.6531e+02, 2.2424e+02],\n",
       "          [6.3368e+01, 1.8782e+02, 6.9085e+01, 1.9211e+02],\n",
       "          [1.5143e+02, 2.1942e+02, 1.5798e+02, 2.3232e+02],\n",
       "          [4.5302e+01, 1.7995e+02, 9.3009e+01, 2.0629e+02],\n",
       "          [4.5983e+01, 1.7382e+02, 1.3717e+02, 2.3771e+02],\n",
       "          [6.5989e+01, 1.9506e+02, 1.2608e+02, 2.3859e+02],\n",
       "          [7.5612e+01, 1.8812e+02, 1.0642e+02, 2.0272e+02],\n",
       "          [1.5630e+02, 2.2624e+02, 1.6693e+02, 2.3921e+02],\n",
       "          [5.7844e+01, 1.8586e+02, 6.9134e+01, 2.0332e+02],\n",
       "          [5.8511e+01, 1.8783e+02, 6.7629e+01, 1.9848e+02],\n",
       "          [5.6321e+01, 1.8569e+02, 6.4863e+01, 2.0169e+02],\n",
       "          [0.0000e+00, 6.0301e+01, 2.3205e+02, 1.6185e+02],\n",
       "          [5.0454e+01, 2.1590e+02, 1.2286e+02, 2.5600e+02],\n",
       "          [1.7616e+02, 2.1761e+02, 1.9852e+02, 2.2833e+02],\n",
       "          [5.0000e+01, 1.6641e+02, 1.0069e+02, 2.5600e+02],\n",
       "          [1.2052e+02, 1.5607e+02, 1.9629e+02, 2.2572e+02],\n",
       "          [1.4384e+02, 2.0688e+02, 1.6170e+02, 2.5600e+02],\n",
       "          [7.3280e+01, 1.7990e+02, 1.2420e+02, 2.1499e+02],\n",
       "          [4.8036e+01, 1.8907e+02, 7.1830e+01, 2.0260e+02],\n",
       "          [8.5656e+01, 1.8717e+02, 9.5641e+01, 1.9746e+02],\n",
       "          [1.4052e+02, 2.1959e+02, 1.6542e+02, 2.3647e+02],\n",
       "          [6.0421e+01, 2.1242e+02, 6.9953e+01, 2.2650e+02],\n",
       "          [6.4029e+01, 2.1857e+02, 7.8199e+01, 2.3091e+02],\n",
       "          [8.6874e+01, 1.8686e+02, 9.8005e+01, 2.0300e+02],\n",
       "          [1.5795e+02, 3.5295e-02, 1.6570e+02, 3.0521e+00],\n",
       "          [1.5705e+02, 1.2387e-01, 1.6610e+02, 1.0533e+01],\n",
       "          [8.2327e+01, 2.0624e+02, 1.1869e+02, 2.4712e+02],\n",
       "          [0.0000e+00, 1.6772e+02, 1.2822e+02, 2.5600e+02],\n",
       "          [9.0987e+01, 2.0972e+01, 1.8277e+02, 1.0991e+02],\n",
       "          [1.7105e+02, 1.7097e+02, 1.8915e+02, 2.2241e+02],\n",
       "          [1.5493e+02, 7.4625e-02, 1.6719e+02, 6.1712e+00],\n",
       "          [5.8430e+01, 2.1796e+02, 6.7662e+01, 2.3110e+02],\n",
       "          [8.5333e+01, 2.0589e+02, 9.9998e+01, 2.5592e+02],\n",
       "          [1.8095e+02, 2.1027e+02, 1.9925e+02, 2.5181e+02],\n",
       "          [5.8886e+01, 2.1996e+02, 6.7152e+01, 2.2470e+02],\n",
       "          [9.0808e+01, 1.8807e+02, 1.0073e+02, 1.9988e+02],\n",
       "          [1.5350e+02, 1.6858e+02, 1.8944e+02, 2.0926e+02],\n",
       "          [3.1323e+01, 1.8357e+02, 9.1129e+01, 2.3205e+02],\n",
       "          [5.1780e+01, 2.0877e+02, 6.9117e+01, 2.5525e+02],\n",
       "          [5.1188e+01, 2.1194e+02, 8.4613e+01, 2.5308e+02],\n",
       "          [1.5077e+02, 1.9034e+02, 1.5846e+02, 2.0751e+02],\n",
       "          [1.7165e+02, 1.8432e+02, 2.0079e+02, 1.9850e+02],\n",
       "          [5.5534e-01, 3.6226e-01, 1.8469e+02, 3.9933e+01],\n",
       "          [5.5429e+01, 2.1898e+02, 6.4110e+01, 2.3061e+02],\n",
       "          [8.8383e+01, 1.8099e+02, 9.6936e+01, 1.9664e+02],\n",
       "          [8.0648e+01, 2.1762e+02, 1.0192e+02, 2.3571e+02],\n",
       "          [4.1629e+01, 2.1151e+02, 9.1967e+01, 2.3550e+02],\n",
       "          [9.2415e+01, 2.1279e+02, 1.0170e+02, 2.2644e+02],\n",
       "          [1.2359e+02, 9.5769e+00, 2.0219e+02, 1.1190e+02],\n",
       "          [1.1413e+02, 2.0265e+02, 2.3753e+02, 2.5600e+02],\n",
       "          [9.6223e+01, 1.8733e+02, 1.0137e+02, 1.9177e+02],\n",
       "          [1.0756e+02, 1.2462e+00, 1.7149e+02, 7.9148e+01],\n",
       "          [8.6680e+01, 2.1866e+02, 9.5612e+01, 2.3080e+02],\n",
       "          [9.0279e+01, 2.1828e+02, 9.9349e+01, 2.3010e+02],\n",
       "          [1.6288e+02, 1.8713e+02, 1.6850e+02, 1.9137e+02],\n",
       "          [9.0142e+01, 2.2069e+02, 9.9403e+01, 2.2522e+02],\n",
       "          [1.5407e+02, 1.8073e+02, 1.6524e+02, 1.9147e+02],\n",
       "          [1.8450e+02, 2.1812e+02, 1.9226e+02, 2.3079e+02],\n",
       "          [5.2353e+01, 1.6428e+02, 6.7007e+01, 2.1053e+02],\n",
       "          [9.1996e+01, 2.2097e+02, 1.0697e+02, 2.3113e+02],\n",
       "          [1.5147e+02, 1.8729e+02, 1.5777e+02, 2.0065e+02],\n",
       "          [5.0028e+01, 2.1795e+02, 6.8165e+01, 2.3515e+02],\n",
       "          [4.5535e+01, 1.6583e+02, 8.0170e+01, 2.0939e+02],\n",
       "          [1.3759e+02, 1.8487e+02, 2.1410e+02, 2.3898e+02],\n",
       "          [1.5619e+02, 1.8650e+02, 2.0267e+02, 2.1938e+02],\n",
       "          [1.5554e+02, 1.8555e+02, 1.6403e+02, 2.0142e+02],\n",
       "          [8.0316e+01, 1.6485e+02, 9.4238e+01, 2.0696e+02],\n",
       "          [1.8641e+02, 2.1958e+02, 1.9725e+02, 2.2461e+02],\n",
       "          [1.0493e+02, 1.8726e+02, 1.3705e+02, 2.0351e+02],\n",
       "          [5.5319e+01, 1.4875e+02, 9.8820e+01, 2.2725e+02],\n",
       "          [1.7768e+02, 1.6826e+02, 2.1840e+02, 2.1774e+02],\n",
       "          [1.2324e+02, 2.1209e+02, 1.3255e+02, 2.2625e+02],\n",
       "          [1.6367e+02, 5.8446e-01, 2.0900e+02, 4.2421e+01],\n",
       "          [1.2226e+02, 2.1737e+02, 1.3241e+02, 2.3104e+02],\n",
       "          [1.2260e+02, 2.2321e+02, 1.3627e+02, 2.3426e+02],\n",
       "          [9.4485e+01, 3.0434e-02, 1.7552e+02, 2.4557e+00],\n",
       "          [9.7815e+01, 1.7856e-01, 1.5060e+02, 2.0777e+01],\n",
       "          [1.6889e+02, 1.3758e-01, 2.0918e+02, 1.2074e+01],\n",
       "          [1.2145e+02, 2.2047e+02, 1.3123e+02, 2.2474e+02],\n",
       "          [1.1119e+02, 5.1785e+01, 1.8649e+02, 9.3702e+01],\n",
       "          [1.8202e+02, 1.8783e+02, 1.9314e+02, 1.9846e+02],\n",
       "          [7.8928e+01, 1.6056e+02, 1.1601e+02, 2.0598e+02],\n",
       "          [4.8867e+01, 4.6825e-01, 1.6732e+02, 6.1812e+01],\n",
       "          [1.0941e+02, 2.1939e+02, 1.3190e+02, 2.3663e+02],\n",
       "          [3.0545e+01, 1.8806e+02, 3.7216e+01, 1.9291e+02],\n",
       "          [1.8749e+02, 2.1521e+02, 1.9682e+02, 2.2691e+02],\n",
       "          [1.2292e+02, 2.0543e+02, 1.5628e+02, 2.4372e+02],\n",
       "          [8.2600e+01, 2.2303e+02, 1.5606e+02, 2.5600e+02],\n",
       "          [2.5223e+01, 1.3827e+02, 2.3466e+02, 2.5600e+02],\n",
       "          [1.1898e+02, 2.1904e+02, 1.2835e+02, 2.3115e+02],\n",
       "          [1.1892e+02, 2.0399e+02, 1.3285e+02, 2.5597e+02],\n",
       "          [1.7540e+02, 2.2375e-01, 1.9652e+02, 2.1447e+01],\n",
       "          [2.7371e+01, 1.8865e+02, 3.5637e+01, 1.9890e+02],\n",
       "          [1.8412e+02, 1.5544e+02, 1.9628e+02, 1.6777e+02],\n",
       "          [1.5312e+02, 2.7584e+01, 1.6049e+02, 3.7382e+01],\n",
       "          [1.5688e+02, 2.6658e+01, 1.6555e+02, 3.8378e+01]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]),\n",
       "  'scores': tensor([0.5392, 0.5385, 0.5376, 0.5373, 0.5371, 0.5366, 0.5364, 0.5357, 0.5356,\n",
       "          0.5356, 0.5353, 0.5350, 0.5349, 0.5346, 0.5343, 0.5343, 0.5336, 0.5336,\n",
       "          0.5334, 0.5321, 0.5318, 0.5313, 0.5313, 0.5304, 0.5304, 0.5300, 0.5299,\n",
       "          0.5297, 0.5295, 0.5294, 0.5290, 0.5288, 0.5288, 0.5287, 0.5284, 0.5282,\n",
       "          0.5280, 0.5280, 0.5279, 0.5279, 0.5278, 0.5277, 0.5277, 0.5272, 0.5271,\n",
       "          0.5268, 0.5268, 0.5268, 0.5266, 0.5265, 0.5264, 0.5263, 0.5263, 0.5263,\n",
       "          0.5262, 0.5262, 0.5262, 0.5260, 0.5258, 0.5257, 0.5257, 0.5256, 0.5255,\n",
       "          0.5255, 0.5255, 0.5250, 0.5248, 0.5242, 0.5241, 0.5240, 0.5239, 0.5236,\n",
       "          0.5231, 0.5231, 0.5230, 0.5230, 0.5229, 0.5227, 0.5226, 0.5224, 0.5223,\n",
       "          0.5221, 0.5221, 0.5219, 0.5219, 0.5219, 0.5218, 0.5216, 0.5215, 0.5215,\n",
       "          0.5215, 0.5214, 0.5212, 0.5212, 0.5211, 0.5208, 0.5205, 0.5202, 0.5201,\n",
       "          0.5201], grad_fn=<IndexBackward>)}]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = Image.fromarray(image[0].numpy()[0, :, :])\n",
    "if image2.mode != \"RGB\":\n",
    "    image2 = image2.convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(image2)\n",
    "for box, score in zip(prediction[0][\"boxes\"], prediction[0][\"scores\"]):\n",
    "    x0, y0, x1, y1  = box\n",
    "    draw.rectangle([(x0, y0), (x1, y1)], outline=(255, 0, 255))\n",
    "\n",
    "image2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = Image.fromarray(image.numpy()[0, 0, :, :])\n",
    "if image3.mode != \"RGB\":\n",
    "    image3 = image3.convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(image3)\n",
    "for box in targets[0][\"boxes\"]:\n",
    "    x0, y0, x1, y1  = box\n",
    "    draw.rectangle([(x0, y0), (x1, y1)], outline=(255, 0, 255))\n",
    "\n",
    "image3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}